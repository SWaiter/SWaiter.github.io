---
layout: post
title: kafka云数据建设01
categories: swaiter
description: linux基础建设
keywords: hive、table、db、join
---

拿到三台服务器，首先对三台机器做免密登录，主要遇到的问题和方式如下：
### 基础


在 Linux 系统上 SSH 是非常常用的工具，通过 SSH Client 我们可以连接到运行了 SSH Server 的远程机器上。SSH Client 的基本使用方法是：

```bash
ssh user@remote -p port
```

- user 是你在远程机器上的用户名，如果不指定的话默认为当前用户
- remote 是远程机器的地址，可以是 IP，域名，或者是后面会提到的别名
- port 是 SSH Server 监听的端口，如果不指定的话就为默认值 22

实际上，知道了上面这三个参数，用任意的 SSH Client 都能连接上 SSH Server，
```bash
local$ ssh user@remote -p port                                                      
user@remote's password:
```
每次操作比较麻烦
### 免密码登入

每次 ssh 都要输入密码是不是很烦呢？与密码验证相对的，是公钥验证。也就是说，要实现免密码登入，首先要设置 SSH 钥匙。

执行 ssh-keygen 即可生成 SSH 钥匙，一路回车即可。
```bash

local$ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/user/.ssh/id_rsa):
Created directory '/home/user/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/user/.ssh/id_rsa.
Your public key has been saved in /home/user/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:47VkvSjlFhKRgz/6RYdXM2EULtk9TQ65PDWJjYC5Jys user@local
The key's randomart image is:
+---[RSA 2048]----+
|       ...o...X+o|
|      . o+   B=Oo|
|       .....ooo*=|
|        o+ooo.+ .|
|       .SoXo.  . |
|      .E X.+ .   |
|       .+.= .    |
|        .o       |
|                 |
+----[SHA256]-----+

```
这段话告诉了我们，生成的公钥放在了 ~/.ssh/id_rsa.pub，私钥放在了 ~/.ssh/id_rsa。接下来，我们要让远程机器记住我们的公钥。
[官方地址](https://hive.apache.org/)

### Hive的基本操作
#### 1.建数据库
```hql
CREATE DATABASE IF NOT EXISTS test
COMMENT '添加对表的描述'
```

#### 2.上传文件至hdfs

```bash
# 把本地数据put到集群
$hadoop fs -put /Users/mrlevo/Desktop/project/163music/music_data  /test/music/
```
#### 3.查看

```bash

# 其中hdfs上的数据是这样的,它会location到该路径下的所有文件
$hadoop fs -ls /test/music/
-rw-r--r--   1 mac supergroup    2827582 2017-07-07 15:03 /test/music/music_data

# music_data里面的文件是这样的

$ hadoop fs -cat /test/music/music_data | more
xxx|9|让音乐串起你我|云南省|文山壮族苗族自治州|75后|新浪微博|482|2002|326
xx|8|None|云南省|曲靖市|75后|None|0|12|4
xx|8|百年云烟只过眼，不为繁华易素心|贵州省|贵阳市|85后|None|1|22|1
```
```bash
# 文件大小
$ hadoop fs -du /t_user/my_hive_db/my_hive_table_test01
17/10/15 12:14:54 INFO hdfs.PeerCache: SocketCache disabled.
282171728  /t_user/my_hive_db/my_hive_table_test01/000000_0
281446475  /t_user/my_hive_db/my_hive_table_test01/000001_0
281021562  /t_user/my_hive_db/my_hive_table_test01/000002_0
280834172  /t_user/my_hive_db/my_hive_table_test01/000003_0
280411919  /t_user/my_hive_db/my_hive_table_test01/000004_0
279749295  /t_user/my_hive_db/my_hive_table_test01/000005_0

$ hadoop fs -du -h /t_user/my_hive_db/my_hive_table_test01
17/10/15 12:17:05 INFO hdfs.PeerCache: SocketCache disabled.
269.1 M  /t_user/my_hive_db/my_hive_table_test01/000000_0
268.4 M  /t_user/my_hive_db/my_hive_table_test01/000001_0
268.0 M  /t_user/my_hive_db/my_hive_table_test01/000002_0
267.8 M  /t_user/my_hive_db/my_hive_table_test01/000003_0
267.4 M  /t_user/my_hive_db/my_hive_table_test01/000004_0
266.8 M  /t_user/my_hive_db/my_hive_table_test01/000005_0

$ hadoop fs -du -s /t_user/my_hive_db/my_hive_table_test01
17/10/15 12:16:52 INFO hdfs.PeerCache: SocketCache disabled.
29959873142  /t_user/my_hive_db/my_hive_table_test01

$ hadoop fs -du -s -h /t_user/my_hive_db/my_hive_table_test01
17/10/15 12:17:17 INFO hdfs.PeerCache: SocketCache disabled.
27.9 G  /t_user/my_hive_db/my_hive_table_test01
```

#### 4.建表
然后再直接建hive表并关联数据
```bash
hive> create external table test.163music(   //建立外表，选择的数据库是test，表是163music
    nickname string,
    stage string,
    introduce string,
    province string,
    city string,
    age string,
    social string,
    trends string,
    follow string,
    fans string,
    homepage string)   //这里是列名
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'   //这里是关联进表里面的数据的分隔符
    LOCATION '/test/music/';    //这里location 选择的是hdfs的路径，比如我把我文件放在hdfs路径是/test/music/
```
如果是内表的话，会默认url，内表和外表有一定的区别，对外表进行drop，其对应的文件保留。

###5.查看数据
```hql
select * from test.163music limit 1;
```

###6.查看表结构
```hql
desc test.163music;
```
结果是：
```hql
nickname                string
stage                   string
introduce               string
province                string
city                    string
age                     string
social                  string
trends                  string
follow                  string
fans                    string
homepage                string
```

